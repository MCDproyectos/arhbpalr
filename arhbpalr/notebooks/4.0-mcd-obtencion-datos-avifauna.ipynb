{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OBTENCI√ìN DE DATOS DE AVIFAUNA - eBird API**\n",
    "\n",
    "Este notebook obtiene los datos hist√≥ricos de avistamientos de aves en la Presa Abelardo L. Rodr√≠guez usando la API de eBird.\n",
    "\n",
    "**Requisitos previos:**\n",
    "- Ejecutar notebooks 1.0, 2.0 y 3.0 para tener el archivo `datos_hidrologicos_completos.csv`\n",
    "- API key de eBird v√°lida\n",
    "\n",
    "**Salida:** \n",
    "- `../data/raw/avistamientos_ebird_raw.csv` (datos crudos de la API)\n",
    "- `../data/processed/avistamientos_aves_presa.csv` (datos limpios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librer√≠as y configurar rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de datos raw: C:\\Users\\Santy\\Documents\\GitHub\\arhbpalr\\arhbpalr\\data\\raw\n",
      "Directorio de datos processed: C:\\Users\\Santy\\Documents\\GitHub\\arhbpalr\\arhbpalr\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configurar rutas siguiendo estructura Cookiecutter\n",
    "project_dir = Path.cwd().parent\n",
    "data_raw = project_dir / 'data' / 'raw'\n",
    "data_processed = project_dir / 'data' / 'processed'\n",
    "\n",
    "# Crear directorios si no existen\n",
    "data_raw.mkdir(parents=True, exist_ok=True)\n",
    "data_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio de datos raw: {data_raw}\")\n",
    "print(f\"Directorio de datos processed: {data_processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de eBird API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuraci√≥n de API completada\n",
      "  Location ID: L506196\n",
      "  Sleep entre requests: 0.5s\n",
      "  Max reintentos: 3\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de la API\n",
    "EBIRD_API_KEY = \"se6b2m6ll8ca\"\n",
    "LOC_ID = \"L506196\"  # Presa Abelardo L. Rodr√≠guez\n",
    "API_BASE_URL = \"https://api.ebird.org/v2/data/obs\"\n",
    "\n",
    "# Headers requeridos por la API\n",
    "HEADERS = {\n",
    "    \"X-eBirdApiToken\": EBIRD_API_KEY\n",
    "}\n",
    "\n",
    "# Configuraci√≥n de throttling y reintentos\n",
    "SLEEP_BETWEEN_REQUESTS = 0.5  # segundos\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF = 2  # exponencial\n",
    "RATE_LIMIT_WAIT = 60  # segundos para esperar si hay rate limit\n",
    "CHECKPOINT_INTERVAL = 100  # Guardar progreso cada N requests\n",
    "\n",
    "print(\"‚úì Configuraci√≥n de API completada\")\n",
    "print(f\"  Location ID: {LOC_ID}\")\n",
    "print(f\"  Sleep entre requests: {SLEEP_BETWEEN_REQUESTS}s\")\n",
    "print(f\"  Max reintentos: {MAX_RETRIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determinar rango de fechas a consultar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANGO DE FECHAS A CONSULTAR\n",
      "======================================================================\n",
      "\n",
      "Fecha m√°s antigua (datos de presa): 1947-04-14\n",
      "Fecha m√°s reciente: 2025-10-11\n",
      "Total de d√≠as a consultar: 28,671\n",
      "\n",
      "Tiempo estimado (con sleep de 0.5s): 3.98 horas\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos hidrol√≥gicos procesados para obtener la fecha m√°s antigua\n",
    "file_hidro = data_processed / 'datos_hidrologicos_completos.csv'\n",
    "\n",
    "if not file_hidro.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå No se encontr√≥ {file_hidro}\\n\"\n",
    "        \"   Ejecuta primero el notebook 3.0-mcd-fusion-datos-hidrologicos.ipynb\"\n",
    "    )\n",
    "\n",
    "df_hidro = pd.read_csv(file_hidro)\n",
    "df_hidro['fecha'] = pd.to_datetime(df_hidro['fecha'])\n",
    "\n",
    "# Filtrar fechas donde hay datos de presa (almacenamiento no es NaN)\n",
    "df_con_presa = df_hidro[df_hidro['almacenamiento_hm3'].notna()]\n",
    "\n",
    "# Obtener rango de fechas\n",
    "fecha_minima = df_con_presa['fecha'].min().date()\n",
    "fecha_maxima = datetime(2025, 10, 11).date()  # Fecha actual especificada\n",
    "\n",
    "# Generar lista de fechas a consultar (desde la m√°s reciente a la m√°s antigua)\n",
    "fechas_a_consultar = []\n",
    "fecha_actual = fecha_maxima\n",
    "while fecha_actual >= fecha_minima:\n",
    "    fechas_a_consultar.append(fecha_actual)\n",
    "    fecha_actual -= timedelta(days=1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RANGO DE FECHAS A CONSULTAR\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFecha m√°s antigua (datos de presa): {fecha_minima}\")\n",
    "print(f\"Fecha m√°s reciente: {fecha_maxima}\")\n",
    "print(f\"Total de d√≠as a consultar: {len(fechas_a_consultar):,}\")\n",
    "print(f\"\\nTiempo estimado (con sleep de {SLEEP_BETWEEN_REQUESTS}s): \"\n",
    "      f\"{len(fechas_a_consultar) * SLEEP_BETWEEN_REQUESTS / 3600:.2f} horas\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones auxiliares para requests a la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funciones auxiliares definidas\n"
     ]
    }
   ],
   "source": [
    "def hacer_request_ebird(\n",
    "    fecha: datetime.date,\n",
    "    loc_id: str = LOC_ID,\n",
    "    headers: dict = HEADERS,\n",
    "    max_retries: int = MAX_RETRIES\n",
    ") -> Optional[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Hace un request a la API de eBird para obtener avistamientos hist√≥ricos.\n",
    "    \n",
    "    Args:\n",
    "        fecha: Fecha a consultar\n",
    "        loc_id: ID de la localizaci√≥n\n",
    "        headers: Headers con API key\n",
    "        max_retries: N√∫mero m√°ximo de reintentos\n",
    "    \n",
    "    Returns:\n",
    "        Lista de avistamientos o None si hay error\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}/{loc_id}/historic/{fecha.year}/{fecha.month}/{fecha.day}\"\n",
    "    \n",
    "    for intento in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            \n",
    "            # Request exitoso\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            \n",
    "            # No hay datos para esta fecha\n",
    "            elif response.status_code == 404:\n",
    "                return []  # Lista vac√≠a indica que no hay avistamientos\n",
    "            \n",
    "            # Rate limit excedido\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"\\n‚ö†Ô∏è  Rate limit alcanzado. Esperando {RATE_LIMIT_WAIT}s...\")\n",
    "                time.sleep(RATE_LIMIT_WAIT)\n",
    "                continue\n",
    "            \n",
    "            # Otros errores HTTP\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error HTTP {response.status_code} para {fecha}: {response.text}\")\n",
    "                if intento < max_retries - 1:\n",
    "                    wait_time = RETRY_BACKOFF ** intento\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                return None\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\\n‚ö†Ô∏è  Timeout para {fecha}. Reintento {intento + 1}/{max_retries}\")\n",
    "            if intento < max_retries - 1:\n",
    "                time.sleep(RETRY_BACKOFF ** intento)\n",
    "                continue\n",
    "            return None\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n‚ùå Error de red para {fecha}: {e}\")\n",
    "            if intento < max_retries - 1:\n",
    "                time.sleep(RETRY_BACKOFF ** intento)\n",
    "                continue\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extraer_campos_relevantes(avistamiento: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Extrae solo los campos relevantes de un avistamiento.\n",
    "    \n",
    "    Args:\n",
    "        avistamiento: Diccionario con datos de avistamiento de la API\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con campos relevantes\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'fecha': avistamiento.get('obsDt'),\n",
    "        'nombre_comun': avistamiento.get('comName'),\n",
    "        'nombre_cientifico': avistamiento.get('sciName'),\n",
    "        'cantidad': avistamiento.get('howMany'),\n",
    "        'codigo_especie': avistamiento.get('speciesCode'),\n",
    "        'es_exotica': avistamiento.get('exoticCategory', None)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint: Verificar si hay progreso guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Checkpoint encontrado. Cargando progreso previo...\n",
      "‚úì Cargadas 100 fechas procesadas previamente\n",
      "‚úì Cargados 243 avistamientos previos\n",
      "‚úì Quedan 28,571 fechas por procesar\n",
      "\n",
      "Total de requests a realizar: 28,571\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = data_raw / 'checkpoint_avistamientos.json'\n",
    "checkpoint_data_file = data_raw / 'avistamientos_checkpoint.csv'\n",
    "\n",
    "# Verificar si hay checkpoint previo\n",
    "fechas_ya_procesadas = set()\n",
    "avistamientos_previos = []\n",
    "\n",
    "if checkpoint_file.exists() and checkpoint_data_file.exists():\n",
    "    print(\"üìÇ Checkpoint encontrado. Cargando progreso previo...\")\n",
    "    \n",
    "    # Cargar checkpoint\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    fechas_ya_procesadas = set(checkpoint.get('fechas_procesadas', []))\n",
    "    \n",
    "    # Cargar datos previos\n",
    "    df_previo = pd.read_csv(checkpoint_data_file)\n",
    "    avistamientos_previos = df_previo.to_dict('records')\n",
    "    \n",
    "    print(f\"‚úì Cargadas {len(fechas_ya_procesadas):,} fechas procesadas previamente\")\n",
    "    print(f\"‚úì Cargados {len(avistamientos_previos):,} avistamientos previos\")\n",
    "    \n",
    "    # Filtrar fechas ya procesadas\n",
    "    fechas_a_consultar = [\n",
    "        f for f in fechas_a_consultar \n",
    "        if f.isoformat() not in fechas_ya_procesadas\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úì Quedan {len(fechas_a_consultar):,} fechas por procesar\")\n",
    "else:\n",
    "    print(\"üìÇ No se encontr√≥ checkpoint previo. Iniciando desde cero.\")\n",
    "\n",
    "print(f\"\\nTotal de requests a realizar: {len(fechas_a_consultar):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loop principal: Obtener datos de la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INICIANDO OBTENCI√ìN DE DATOS DE eBird API\n",
      "======================================================================\n",
      "\n",
      "Fechas a procesar: 28,671\n",
      "Checkpoint cada: 100 requests\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703146c4daec4302a8c868120003e033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obteniendo avistamientos:   0%|          | 0/28671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Especies √∫nicas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(stats[\u001b[33m'\u001b[39m\u001b[33mespecies_unicas\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Sleep para respetar rate limits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSLEEP_BETWEEN_REQUESTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Estad√≠sticas finales\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Listas para almacenar resultados\n",
    "todos_avistamientos = avistamientos_previos.copy()\n",
    "fechas_procesadas = fechas_ya_procesadas.copy()\n",
    "\n",
    "# Contadores de estad√≠sticas\n",
    "stats = {\n",
    "    'exitosos': 0,\n",
    "    'sin_datos': 0,\n",
    "    'errores': 0,\n",
    "    'especies_unicas': set()\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INICIANDO OBTENCI√ìN DE DATOS DE eBird API\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFechas a procesar: {len(fechas_a_consultar):,}\")\n",
    "print(f\"Checkpoint cada: {CHECKPOINT_INTERVAL} requests\\n\")\n",
    "\n",
    "# Loop principal con barra de progreso\n",
    "for idx, fecha in enumerate(tqdm(fechas_a_consultar, desc=\"Obteniendo avistamientos\")):\n",
    "    \n",
    "    # Hacer request\n",
    "    avistamientos = hacer_request_ebird(fecha)\n",
    "    \n",
    "    # Procesar resultados\n",
    "    if avistamientos is None:\n",
    "        stats['errores'] += 1\n",
    "    elif len(avistamientos) == 0:\n",
    "        stats['sin_datos'] += 1\n",
    "    else:\n",
    "        stats['exitosos'] += 1\n",
    "        \n",
    "        # Extraer campos relevantes\n",
    "        for avistamiento in avistamientos:\n",
    "            dato_limpio = extraer_campos_relevantes(avistamiento)\n",
    "            todos_avistamientos.append(dato_limpio)\n",
    "            \n",
    "            # Actualizar especies √∫nicas\n",
    "            if dato_limpio['codigo_especie']:\n",
    "                stats['especies_unicas'].add(dato_limpio['codigo_especie'])\n",
    "    \n",
    "    # Marcar fecha como procesada\n",
    "    fechas_procesadas.add(fecha.isoformat())\n",
    "    \n",
    "    # Guardar checkpoint peri√≥dicamente\n",
    "    if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "        # Guardar checkpoint\n",
    "        checkpoint_data = {\n",
    "            'fechas_procesadas': list(fechas_procesadas),\n",
    "            'ultima_actualizacion': datetime.now().isoformat(),\n",
    "            'stats': {\n",
    "                'exitosos': stats['exitosos'],\n",
    "                'sin_datos': stats['sin_datos'],\n",
    "                'errores': stats['errores'],\n",
    "                'total_avistamientos': len(todos_avistamientos),\n",
    "                'especies_unicas': len(stats['especies_unicas'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f)\n",
    "        \n",
    "        # Guardar datos\n",
    "        df_checkpoint = pd.DataFrame(todos_avistamientos)\n",
    "        df_checkpoint.to_csv(checkpoint_data_file, index=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Checkpoint guardado en request {idx + 1}\")\n",
    "        print(f\"   Avistamientos totales: {len(todos_avistamientos):,}\")\n",
    "        print(f\"   Especies √∫nicas: {len(stats['especies_unicas'])}\")\n",
    "    \n",
    "    # Sleep para respetar rate limits\n",
    "    time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBTENCI√ìN COMPLETADA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úì Requests exitosos: {stats['exitosos']:,}\")\n",
    "print(f\"  Requests sin datos: {stats['sin_datos']:,}\")\n",
    "print(f\"‚ùå Requests con error: {stats['errores']:,}\")\n",
    "print(f\"\\nüê¶ Total de avistamientos: {len(todos_avistamientos):,}\")\n",
    "print(f\"ü¶Ö Especies √∫nicas: {len(stats['especies_unicas'])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardar datos crudos (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'todos_avistamientos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convertir a DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_raw = pd.DataFrame(\u001b[43mtodos_avistamientos\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Guardar datos crudos\u001b[39;00m\n\u001b[32m      5\u001b[39m output_raw = data_raw / \u001b[33m'\u001b[39m\u001b[33mavistamientos_ebird_raw.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'todos_avistamientos' is not defined"
     ]
    }
   ],
   "source": [
    "# Convertir a DataFrame\n",
    "df_raw = pd.DataFrame(todos_avistamientos)\n",
    "\n",
    "# Guardar datos crudos\n",
    "output_raw = data_raw / 'avistamientos_ebird_raw.csv'\n",
    "df_raw.to_csv(output_raw, index=False)\n",
    "\n",
    "print(\"‚úì Datos crudos guardados\")\n",
    "print(f\"  Archivo: {output_raw}\")\n",
    "print(f\"  Registros: {len(df_raw):,}\")\n",
    "print(f\"  Tama√±o: {output_raw.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Limpiar archivos de checkpoint\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()\n",
    "if checkpoint_data_file.exists():\n",
    "    checkpoint_data_file.unlink()\n",
    "print(\"\\nüßπ Archivos de checkpoint eliminados\")\n",
    "\n",
    "print(\"\\nPrimeros registros:\")\n",
    "display(df_raw.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limpieza y procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LIMPIEZA Y PROCESAMIENTO DE DATOS\n",
      "======================================================================\n",
      "\n",
      "‚úì Fechas convertidas a datetime\n",
      "‚úì Columna fecha_solo creada\n",
      "‚úì Cantidades convertidas a num√©rico\n",
      "‚úì Flag de especies ex√≥ticas creado\n",
      "\n",
      "Valores faltantes por columna:\n",
      "\n",
      "‚úì Eliminados 0 registros sin nombre cient√≠fico\n",
      "‚úì Datos ordenados por fecha\n",
      "\n",
      "======================================================================\n",
      "Registros despu√©s de limpieza: 4,854\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LIMPIEZA Y PROCESAMIENTO DE DATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear copia para procesar\n",
    "df_processed = df_raw.copy()\n",
    "\n",
    "# 1. Convertir fecha a datetime\n",
    "df_processed['fecha'] = pd.to_datetime(df_processed['fecha'], format='mixed')\n",
    "print(f\"\\n‚úì Fechas convertidas a datetime\")\n",
    "\n",
    "# 2. Extraer solo la fecha (sin hora) para agregaci√≥n\n",
    "df_processed['fecha_solo'] = df_processed['fecha'].dt.date\n",
    "print(f\"‚úì Columna fecha_solo creada\")\n",
    "\n",
    "# 3. Convertir cantidad a num√©rico (puede venir como string)\n",
    "df_processed['cantidad'] = pd.to_numeric(df_processed['cantidad'], errors='coerce').fillna(1).astype(int)\n",
    "print(f\"‚úì Cantidades convertidas a num√©rico\")\n",
    "\n",
    "# 4. Crear flag para especies ex√≥ticas\n",
    "df_processed['es_exotica'] = df_processed['es_exotica'].notna()\n",
    "print(f\"‚úì Flag de especies ex√≥ticas creado\")\n",
    "\n",
    "# 5. Verificar valores faltantes\n",
    "print(f\"\\nValores faltantes por columna:\")\n",
    "missing = df_processed.isnull().sum()\n",
    "for col, count in missing[missing > 0].items():\n",
    "    print(f\"  {col}: {count:,} ({count/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# 6. Eliminar registros sin nombre cient√≠fico (calidad de datos)\n",
    "len_antes = len(df_processed)\n",
    "df_processed = df_processed[df_processed['nombre_cientifico'].notna()]\n",
    "len_despues = len(df_processed)\n",
    "print(f\"\\n‚úì Eliminados {len_antes - len_despues:,} registros sin nombre cient√≠fico\")\n",
    "\n",
    "# 7. Ordenar por fecha\n",
    "df_processed = df_processed.sort_values('fecha').reset_index(drop=True)\n",
    "print(f\"‚úì Datos ordenados por fecha\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Registros despu√©s de limpieza: {len(df_processed):,}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar dataset procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ DATASET PROCESADO GUARDADO EXITOSAMENTE\n",
      "======================================================================\n",
      "\n",
      "Archivo: C:\\Users\\Santy\\Documents\\GitHub\\arhbpalr\\arhbpalr\\data\\processed\\avistamientos_aves_presa.csv\n",
      "Registros: 4,854\n",
      "Columnas: 6\n",
      "Tama√±o: 0.33 MB\n",
      "\n",
      "Rango temporal: 1982-11-06 a 2025-10-11\n",
      "\n",
      "Columnas incluidas:\n",
      "  1. fecha\n",
      "  2. nombre_comun\n",
      "  3. nombre_cientifico\n",
      "  4. codigo_especie\n",
      "  5. cantidad\n",
      "  6. es_exotica\n",
      "\n",
      "======================================================================\n",
      "üéâ OBTENCI√ìN Y PROCESAMIENTO DE DATOS DE AVIFAUNA COMPLETADO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar columnas finales para el dataset procesado\n",
    "columnas_finales = [\n",
    "    'fecha',\n",
    "    'nombre_comun',\n",
    "    'nombre_cientifico',\n",
    "    'codigo_especie',\n",
    "    'cantidad',\n",
    "    'es_exotica'\n",
    "]\n",
    "\n",
    "df_final = df_processed[columnas_finales].copy()\n",
    "\n",
    "# Guardar en data/processed\n",
    "output_processed = data_processed / 'avistamientos_aves_presa.csv'\n",
    "df_final.to_csv(output_processed, index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ DATASET PROCESADO GUARDADO EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nArchivo: {output_processed}\")\n",
    "print(f\"Registros: {len(df_final):,}\")\n",
    "print(f\"Columnas: {len(df_final.columns)}\")\n",
    "print(f\"Tama√±o: {output_processed.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nRango temporal: {df_final['fecha'].min().date()} a {df_final['fecha'].max().date()}\")\n",
    "print(f\"\\nColumnas incluidas:\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ OBTENCI√ìN Y PROCESAMIENTO DE DATOS DE AVIFAUNA COMPLETADO\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
