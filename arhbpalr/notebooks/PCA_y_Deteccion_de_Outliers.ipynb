{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Componentes Principales (PCA) y Detección de Outliers\n",
    "## Datos Hidrológicos Completos\n",
    "\n",
    "**Objetivo:** Aplicar técnicas de reducción de dimensionalidad (PCA) y detección de valores atípicos a los datos hidrológicos completos.\n",
    "\n",
    "**Dataset:** `datos_hidrologicos_completos.csv` (1940-2024, ~31,000 registros)\n",
    "\n",
    "**Metodología:**\n",
    "1. Análisis Exploratorio de Datos (EDA)\n",
    "2. Preprocesamiento de datos\n",
    "3. Análisis de Componentes Principales (PCA)\n",
    "4. Detección de Outliers (múltiples métodos)\n",
    "5. Visualización y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Librerías básicas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Librerías para PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Librerías para detección de outliers\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom scipy import stats\n\n# Configuración de visualización\nimport matplotlib\n# Configurar backend para visualización\ntry:\n    get_ipython().run_line_magic('matplotlib', 'inline')\n    print(\"Usando backend: inline\")\nexcept:\n    # Si falla, intentar con TkAgg para entornos de escritorio\n    try:\n        matplotlib.use('TkAgg')\n        print(\"Usando backend: TkAgg\")\n    except:\n        print(f\"Usando backend por defecto: {matplotlib.get_backend()}\")\n\n# Configurar estilo\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept:\n    try:\n        plt.style.use('seaborn-darkgrid')\n    except:\n        plt.style.use('default')\n        print(\"Usando estilo por defecto\")\n\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\nprint(\"✓ Librerías importadas exitosamente\")\nprint(f\"✓ Matplotlib backend activo: {matplotlib.get_backend()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar y Explorar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv('../data/processed/datos_hidrologicos_completos.csv')\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"\\nPeriodo de datos: {df['fecha'].min()} a {df['fecha'].max()}\")\n",
    "print(f\"Total de días: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Faltantes': missing_values,\n",
    "    'Porcentaje (%)': missing_percent\n",
    "}).sort_values(by='Valores Faltantes', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Valores Faltantes'] > 0])\n",
    "\n",
    "# Visualizar valores faltantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_data = missing_df[missing_df['Valores Faltantes'] > 0]\n",
    "if len(missing_data) > 0:\n",
    "    plt.barh(missing_data.index, missing_data['Porcentaje (%)'])\n",
    "    plt.xlabel('Porcentaje de Valores Faltantes (%)')\n",
    "    plt.title('Valores Faltantes por Variable')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n¡No hay valores faltantes en el dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo variables numéricas (excluyendo fecha y datos_completos)\n",
    "# Nota: almacenamiento_hm3 tiene muchos valores faltantes, lo excluiremos del análisis PCA\n",
    "\n",
    "print(\"Seleccionando variables numéricas para el análisis...\")\n",
    "\n",
    "# Columnas a excluir\n",
    "exclude_cols = ['fecha', 'datos_completos', 'almacenamiento_hm3']\n",
    "\n",
    "# Seleccionar columnas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nVariables seleccionadas para el análisis ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Crear dataset para análisis\n",
    "df_analysis = df[numeric_cols].copy()\n",
    "\n",
    "# Verificar si hay valores faltantes\n",
    "print(f\"\\nValores faltantes en el dataset de análisis: {df_analysis.isnull().sum().sum()}\")\n",
    "\n",
    "# Si hay valores faltantes, imputar con la mediana\n",
    "if df_analysis.isnull().sum().sum() > 0:\n",
    "    print(\"Imputando valores faltantes con la mediana...\")\n",
    "    df_analysis = df_analysis.fillna(df_analysis.median())\n",
    "    print(\"Imputación completada.\")\n",
    "\n",
    "print(f\"\\nDimensiones del dataset de análisis: {df_analysis.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuciones de las variables\n",
    "print(\"Generando gráficos de distribución...\")\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    axes[i].hist(df_analysis[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(col, fontsize=9)\n",
    "    axes[i].set_xlabel('Valor')\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Ocultar ejes sobrantes\n",
    "for i in range(len(numeric_cols), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Distribución de Variables Numéricas', fontsize=14, y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "print(\"Calculando matriz de correlación...\")\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = df_analysis.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlación - Variables Hidrológicas', fontsize=14, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar correlaciones fuertes\n",
    "print(\"\\nCorrelaciones fuertes (|r| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr, columns=['Variable 1', 'Variable 2', 'Correlación'])\n",
    "high_corr_df = high_corr_df.sort_values(by='Correlación', ascending=False, key=abs)\n",
    "print(high_corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de datos (requerida para PCA)\n",
    "print(\"Estandarizando datos...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_analysis)\n",
    "df_scaled_df = pd.DataFrame(df_scaled, columns=numeric_cols, index=df_analysis.index)\n",
    "\n",
    "print(f\"Datos estandarizados: {df_scaled_df.shape}\")\n",
    "print(\"\\nEstadísticas después de estandarización (deberían tener media ~0 y std ~1):\")\n",
    "print(df_scaled_df.describe().loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Componentes Principales (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA con todas las componentes\n",
    "print(\"Aplicando PCA...\")\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(df_scaled)\n",
    "\n",
    "# Varianza explicada\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\nNúmero total de componentes: {len(explained_variance)}\")\n",
    "print(f\"Varianza explicada por las primeras 5 componentes:\")\n",
    "for i in range(min(5, len(explained_variance))):\n",
    "    print(f\"  PC{i+1}: {explained_variance[i]:.4f} ({explained_variance[i]*100:.2f}%)\")\n",
    "print(f\"\\nVarianza acumulada con 5 componentes: {cumulative_variance[4]:.4f} ({cumulative_variance[4]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de varianza explicada (Scree Plot)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Varianza individual\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Componente Principal')\n",
    "axes[0].set_ylabel('Varianza Explicada')\n",
    "axes[0].set_title('Varianza Explicada por Componente (Scree Plot)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', linewidth=2)\n",
    "axes[1].axhline(y=0.80, color='r', linestyle='--', label='80% varianza')\n",
    "axes[1].axhline(y=0.90, color='g', linestyle='--', label='90% varianza')\n",
    "axes[1].axhline(y=0.95, color='b', linestyle='--', label='95% varianza')\n",
    "axes[1].set_xlabel('Número de Componentes')\n",
    "axes[1].set_ylabel('Varianza Acumulada')\n",
    "axes[1].set_title('Varianza Explicada Acumulada')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar número de componentes para 80%, 90%, 95% de varianza\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nComponentes necesarias para:\")\n",
    "print(f\"  80% de varianza: {n_components_80} componentes\")\n",
    "print(f\"  90% de varianza: {n_components_90} componentes\")\n",
    "print(f\"  95% de varianza: {n_components_95} componentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA con número óptimo de componentes (usaremos 90% de varianza)\n",
    "optimal_components = n_components_90\n",
    "print(f\"Aplicando PCA con {optimal_components} componentes (captura ~90% de varianza)...\")\n",
    "\n",
    "pca = PCA(n_components=optimal_components)\n",
    "pca_data = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Crear DataFrame con componentes principales\n",
    "pca_columns = [f'PC{i+1}' for i in range(optimal_components)]\n",
    "df_pca = pd.DataFrame(pca_data, columns=pca_columns, index=df_analysis.index)\n",
    "\n",
    "# Agregar fecha al DataFrame PCA\n",
    "df_pca['fecha'] = df['fecha'].values\n",
    "\n",
    "print(f\"\\nDimensiones del dataset PCA: {df_pca.shape}\")\n",
    "print(f\"Varianza total explicada: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de loadings (pesos de las variables en cada componente)\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings,\n",
    "    columns=pca_columns,\n",
    "    index=numeric_cols\n",
    ")\n",
    "\n",
    "print(\"\\nLOADINGS DE LAS PRIMERAS 3 COMPONENTES PRINCIPALES\")\n",
    "print(\"=\" * 80)\n",
    "print(loadings_df.iloc[:, :3].to_string())\n",
    "\n",
    "# Visualizar loadings de las primeras 3 componentes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    pc = f'PC{i+1}'\n",
    "    loadings_sorted = loadings_df[pc].sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    axes[i].barh(range(len(loadings_sorted)), loadings_sorted.values, \n",
    "                 color=['red' if x < 0 else 'blue' for x in loadings_sorted.values],\n",
    "                 alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_yticks(range(len(loadings_sorted)))\n",
    "    axes[i].set_yticklabels(loadings_sorted.index, fontsize=8)\n",
    "    axes[i].set_xlabel('Loading')\n",
    "    axes[i].set_title(f'{pc} ({pca.explained_variance_ratio_[i]*100:.1f}% varianza)')\n",
    "    axes[i].axvline(x=0, color='black', linewidth=0.5)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Loadings de las Primeras 3 Componentes Principales', fontsize=14, y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot: PC1 vs PC2\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Scatter plot de las observaciones (muestrear para mejor visualización)\n",
    "sample_size = min(5000, len(df_pca))\n",
    "sample_indices = np.random.choice(len(df_pca), sample_size, replace=False)\n",
    "ax.scatter(df_pca.iloc[sample_indices]['PC1'], \n",
    "           df_pca.iloc[sample_indices]['PC2'], \n",
    "           alpha=0.3, s=10)\n",
    "\n",
    "# Agregar vectores de loadings\n",
    "scale_factor = 3  # Factor para visualizar mejor los vectores\n",
    "for i, var in enumerate(numeric_cols):\n",
    "    ax.arrow(0, 0, \n",
    "             loadings_df.loc[var, 'PC1'] * scale_factor, \n",
    "             loadings_df.loc[var, 'PC2'] * scale_factor,\n",
    "             head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)\n",
    "    ax.text(loadings_df.loc[var, 'PC1'] * scale_factor * 1.15, \n",
    "            loadings_df.loc[var, 'PC2'] * scale_factor * 1.15,\n",
    "            var, fontsize=8, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% varianza)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% varianza)')\n",
    "ax.set_title('Biplot: PC1 vs PC2', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las primeras 3 componentes principales en 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Muestrear datos para mejor visualización\n",
    "sample_size = min(5000, len(df_pca))\n",
    "sample_indices = np.random.choice(len(df_pca), sample_size, replace=False)\n",
    "\n",
    "scatter = ax.scatter(df_pca.iloc[sample_indices]['PC1'],\n",
    "                     df_pca.iloc[sample_indices]['PC2'],\n",
    "                     df_pca.iloc[sample_indices]['PC3'],\n",
    "                     c=df_pca.iloc[sample_indices].index,\n",
    "                     cmap='viridis', alpha=0.5, s=10)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.1f}%)')\n",
    "ax.set_title('Visualización 3D: PC1, PC2, PC3', fontsize=14)\n",
    "\n",
    "plt.colorbar(scatter, label='Índice temporal', pad=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detección de Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Método 1: Z-Score (Distancia de Mahalanobis simplificada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando Z-score en datos estandarizados\n",
    "print(\"Método 1: Detección de Outliers usando Z-Score\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular Z-score para cada variable\n",
    "z_scores = np.abs(stats.zscore(df_analysis))\n",
    "\n",
    "# Umbral: puntos con Z-score > 3 en cualquier variable\n",
    "threshold = 3\n",
    "outliers_zscore = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "print(f\"Umbral Z-score: {threshold}\")\n",
    "print(f\"Outliers detectados: {outliers_zscore.sum()} ({outliers_zscore.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Observaciones normales: {(~outliers_zscore).sum()} ({(~outliers_zscore).sum()/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Método 2: Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando Isolation Forest\n",
    "print(\"Método 2: Detección de Outliers usando Isolation Forest\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configurar Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # Esperamos ~5% de outliers\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Aplicar en datos estandarizados\n",
    "outliers_iso = iso_forest.fit_predict(df_scaled)\n",
    "outliers_iso_bool = outliers_iso == -1  # -1 indica outlier\n",
    "\n",
    "# Scores de anomalía (más negativo = más anómalo)\n",
    "anomaly_scores_iso = iso_forest.score_samples(df_scaled)\n",
    "\n",
    "print(f\"Outliers detectados: {outliers_iso_bool.sum()} ({outliers_iso_bool.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Observaciones normales: {(~outliers_iso_bool).sum()} ({(~outliers_iso_bool).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"\\nScore de anomalía (más negativo = más anómalo):\")\n",
    "print(f\"  Mínimo: {anomaly_scores_iso.min():.4f}\")\n",
    "print(f\"  Máximo: {anomaly_scores_iso.max():.4f}\")\n",
    "print(f\"  Media: {anomaly_scores_iso.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Método 3: Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando Local Outlier Factor (LOF)\n",
    "print(\"Método 3: Detección de Outliers usando Local Outlier Factor (LOF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configurar LOF\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.05  # Esperamos ~5% de outliers\n",
    ")\n",
    "\n",
    "# Aplicar en datos estandarizados\n",
    "outliers_lof = lof.fit_predict(df_scaled)\n",
    "outliers_lof_bool = outliers_lof == -1  # -1 indica outlier\n",
    "\n",
    "# Scores negativos de LOF (más negativo = más anómalo)\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "print(f\"Outliers detectados: {outliers_lof_bool.sum()} ({outliers_lof_bool.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Observaciones normales: {(~outliers_lof_bool).sum()} ({(~outliers_lof_bool).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"\\nLOF Score (más negativo = más anómalo):\")\n",
    "print(f\"  Mínimo: {lof_scores.min():.4f}\")\n",
    "print(f\"  Máximo: {lof_scores.max():.4f}\")\n",
    "print(f\"  Media: {lof_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Método 4: Elliptic Envelope (Asumiendo distribución Gaussiana multivariada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando Elliptic Envelope\n",
    "print(\"Método 4: Detección de Outliers usando Elliptic Envelope\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configurar Elliptic Envelope\n",
    "elliptic = EllipticEnvelope(\n",
    "    contamination=0.05,  # Esperamos ~5% de outliers\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar en datos estandarizados\n",
    "outliers_elliptic = elliptic.fit_predict(df_scaled)\n",
    "outliers_elliptic_bool = outliers_elliptic == -1  # -1 indica outlier\n",
    "\n",
    "# Scores de Mahalanobis distance\n",
    "mahalanobis_scores = elliptic.score_samples(df_scaled)\n",
    "\n",
    "print(f\"Outliers detectados: {outliers_elliptic_bool.sum()} ({outliers_elliptic_bool.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Observaciones normales: {(~outliers_elliptic_bool).sum()} ({(~outliers_elliptic_bool).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"\\nMahalanobis Distance Score (más negativo = más anómalo):\")\n",
    "print(f\"  Mínimo: {mahalanobis_scores.min():.4f}\")\n",
    "print(f\"  Máximo: {mahalanobis_scores.max():.4f}\")\n",
    "print(f\"  Media: {mahalanobis_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Método 5: Outliers en el espacio PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers en el espacio PCA usando distancia euclidiana\n",
    "print(\"Método 5: Detección de Outliers en el Espacio PCA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular distancia euclidiana desde el origen en el espacio PCA\n",
    "pca_distances = np.sqrt(np.sum(pca_data**2, axis=1))\n",
    "\n",
    "# Usar percentil 95 como umbral\n",
    "threshold_pca = np.percentile(pca_distances, 95)\n",
    "outliers_pca_bool = pca_distances > threshold_pca\n",
    "\n",
    "print(f\"Umbral (percentil 95): {threshold_pca:.4f}\")\n",
    "print(f\"Outliers detectados: {outliers_pca_bool.sum()} ({outliers_pca_bool.sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Observaciones normales: {(~outliers_pca_bool).sum()} ({(~outliers_pca_bool).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"\\nDistancia PCA:\")\n",
    "print(f\"  Mínima: {pca_distances.min():.4f}\")\n",
    "print(f\"  Máxima: {pca_distances.max():.4f}\")\n",
    "print(f\"  Media: {pca_distances.mean():.4f}\")\n",
    "print(f\"  Mediana: {np.median(pca_distances):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Comparación de Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con todos los resultados\n",
    "outliers_comparison = pd.DataFrame({\n",
    "    'fecha': df['fecha'],\n",
    "    'Z-Score': outliers_zscore,\n",
    "    'Isolation_Forest': outliers_iso_bool,\n",
    "    'LOF': outliers_lof_bool,\n",
    "    'Elliptic_Envelope': outliers_elliptic_bool,\n",
    "    'PCA_Distance': outliers_pca_bool\n",
    "})\n",
    "\n",
    "# Agregar columna de consenso (outlier si al menos 3 métodos lo detectan)\n",
    "outliers_comparison['Consenso_3+'] = (\n",
    "    outliers_comparison[['Z-Score', 'Isolation_Forest', 'LOF', 'Elliptic_Envelope', 'PCA_Distance']].sum(axis=1) >= 3\n",
    ")\n",
    "\n",
    "print(\"\\nCOMPARACIÓN DE MÉTODOS DE DETECCIÓN DE OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNúmero de outliers detectados por cada método:\")\n",
    "for col in ['Z-Score', 'Isolation_Forest', 'LOF', 'Elliptic_Envelope', 'PCA_Distance', 'Consenso_3+']:\n",
    "    count = outliers_comparison[col].sum()\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {col:25s}: {count:6d} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión entre métodos\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "methods = ['Z-Score', 'Isolation_Forest', 'LOF', 'Elliptic_Envelope', 'PCA_Distance']\n",
    "n_methods = len(methods)\n",
    "\n",
    "fig, axes = plt.subplots(n_methods, n_methods, figsize=(16, 16))\n",
    "\n",
    "for i, method1 in enumerate(methods):\n",
    "    for j, method2 in enumerate(methods):\n",
    "        if i == j:\n",
    "            # Diagonal: mostrar distribución del método\n",
    "            axes[i, j].hist([0, 1], weights=[\n",
    "                (~outliers_comparison[method1]).sum(),\n",
    "                outliers_comparison[method1].sum()\n",
    "            ], bins=2, edgecolor='black', alpha=0.7)\n",
    "            axes[i, j].set_xticks([0.25, 0.75])\n",
    "            axes[i, j].set_xticklabels(['Normal', 'Outlier'])\n",
    "            axes[i, j].set_title(method1, fontsize=10)\n",
    "        else:\n",
    "            # Fuera de diagonal: matriz de confusión\n",
    "            cm = confusion_matrix(outliers_comparison[method1], outliers_comparison[method2])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i, j],\n",
    "                       xticklabels=['Normal', 'Outlier'],\n",
    "                       yticklabels=['Normal', 'Outlier'])\n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel(method1, fontsize=9)\n",
    "            if i == n_methods - 1:\n",
    "                axes[i, j].set_xlabel(method2, fontsize=9)\n",
    "\n",
    "plt.suptitle('Comparación entre Métodos de Detección de Outliers', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de Venn simplificado (usando consenso)\n",
    "print(\"\\nAnálisis de Consenso:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar cuántos métodos detectan cada observación como outlier\n",
    "outlier_counts = outliers_comparison[methods].sum(axis=1)\n",
    "\n",
    "print(\"\\nDistribución de detecciones:\")\n",
    "for i in range(6):\n",
    "    count = (outlier_counts == i).sum()\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  Detectado por {i} métodos: {count:6d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Visualizar distribución\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts = [((outlier_counts == i).sum()) for i in range(6)]\n",
    "plt.bar(range(6), counts, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Número de Métodos que Detectan como Outlier')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Consenso entre Métodos de Detección de Outliers')\n",
    "plt.xticks(range(6))\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Agregar línea vertical en consenso 3+\n",
    "plt.axvline(x=2.5, color='r', linestyle='--', linewidth=2, label='Umbral Consenso (3+ métodos)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualización de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar outliers en el espacio PCA (PC1 vs PC2)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "outlier_methods = [\n",
    "    ('Z-Score', outliers_zscore),\n",
    "    ('Isolation_Forest', outliers_iso_bool),\n",
    "    ('LOF', outliers_lof_bool),\n",
    "    ('Elliptic_Envelope', outliers_elliptic_bool),\n",
    "    ('PCA_Distance', outliers_pca_bool),\n",
    "    ('Consenso_3+', outliers_comparison['Consenso_3+'])\n",
    "]\n",
    "\n",
    "for idx, (method_name, outlier_mask) in enumerate(outlier_methods):\n",
    "    # Plotear normales\n",
    "    axes[idx].scatter(df_pca.loc[~outlier_mask, 'PC1'], \n",
    "                     df_pca.loc[~outlier_mask, 'PC2'],\n",
    "                     c='blue', s=10, alpha=0.3, label='Normal')\n",
    "    # Plotear outliers\n",
    "    axes[idx].scatter(df_pca.loc[outlier_mask, 'PC1'], \n",
    "                     df_pca.loc[outlier_mask, 'PC2'],\n",
    "                     c='red', s=30, alpha=0.7, marker='x', label='Outlier')\n",
    "    \n",
    "    axes[idx].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    axes[idx].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    axes[idx].set_title(f'{method_name}\\n({outlier_mask.sum()} outliers, {outlier_mask.sum()/len(df)*100:.2f}%)')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Outliers en el Espacio PCA (PC1 vs PC2)', fontsize=14, y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización 3D de outliers en espacio PCA\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "for idx, (method_name, outlier_mask) in enumerate(outlier_methods[:4], 1):\n",
    "    ax = fig.add_subplot(2, 2, idx, projection='3d')\n",
    "    \n",
    "    # Normales\n",
    "    ax.scatter(df_pca.loc[~outlier_mask, 'PC1'],\n",
    "               df_pca.loc[~outlier_mask, 'PC2'],\n",
    "               df_pca.loc[~outlier_mask, 'PC3'],\n",
    "               c='blue', s=5, alpha=0.3, label='Normal')\n",
    "    \n",
    "    # Outliers\n",
    "    ax.scatter(df_pca.loc[outlier_mask, 'PC1'],\n",
    "               df_pca.loc[outlier_mask, 'PC2'],\n",
    "               df_pca.loc[outlier_mask, 'PC3'],\n",
    "               c='red', s=30, alpha=0.8, marker='x', label='Outlier')\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]*100:.1f}%)')\n",
    "    ax.set_title(f'{method_name}\\n({outlier_mask.sum()} outliers)')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Outliers en el Espacio PCA 3D', fontsize=14, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización temporal de outliers (usando consenso)\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plotear todas las observaciones\n",
    "ax.scatter(outliers_comparison['fecha'], \n",
    "           range(len(outliers_comparison)),\n",
    "           c=outliers_comparison['Consenso_3+'].astype(int),\n",
    "           cmap='RdYlBu_r', s=1, alpha=0.5)\n",
    "\n",
    "# Marcar outliers por consenso\n",
    "outlier_dates = outliers_comparison[outliers_comparison['Consenso_3+']]['fecha']\n",
    "outlier_indices = outliers_comparison[outliers_comparison['Consenso_3+']].index\n",
    "\n",
    "ax.scatter(outlier_dates, outlier_indices, \n",
    "           c='red', s=20, alpha=0.7, marker='x', label='Outlier (Consenso 3+)')\n",
    "\n",
    "ax.set_xlabel('Fecha')\n",
    "ax.set_ylabel('Índice de Observación')\n",
    "ax.set_title(f'Distribución Temporal de Outliers (Consenso 3+ métodos)\\nTotal: {len(outlier_dates)} outliers')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrimeros 10 outliers detectados por consenso:\")\n",
    "print(outliers_comparison[outliers_comparison['Consenso_3+']][['fecha']].head(10).to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de las variables originales en outliers detectados por consenso\n",
    "outlier_indices_consensus = outliers_comparison[outliers_comparison['Consenso_3+']].index\n",
    "normal_indices_consensus = outliers_comparison[~outliers_comparison['Consenso_3+']].index\n",
    "\n",
    "# Comparar estadísticas entre outliers y normales\n",
    "print(\"\\nCOMPARACIÓN DE VARIABLES: OUTLIERS vs NORMALES (Consenso 3+)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_stats = pd.DataFrame({\n",
    "    'Normal_Media': df_analysis.loc[normal_indices_consensus].mean(),\n",
    "    'Outlier_Media': df_analysis.loc[outlier_indices_consensus].mean(),\n",
    "    'Normal_Std': df_analysis.loc[normal_indices_consensus].std(),\n",
    "    'Outlier_Std': df_analysis.loc[outlier_indices_consensus].std(),\n",
    "    'Diferencia_Media': df_analysis.loc[outlier_indices_consensus].mean() - df_analysis.loc[normal_indices_consensus].mean()\n",
    "})\n",
    "\n",
    "comparison_stats['Diferencia_%'] = (comparison_stats['Diferencia_Media'] / comparison_stats['Normal_Media']) * 100\n",
    "\n",
    "print(comparison_stats.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots comparativos para variables seleccionadas\n",
    "# Seleccionar variables con mayor diferencia\n",
    "top_diff_vars = comparison_stats.sort_values(by='Diferencia_%', key=abs, ascending=False).head(8).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(top_diff_vars):\n",
    "    data_to_plot = [\n",
    "        df_analysis.loc[normal_indices_consensus, var],\n",
    "        df_analysis.loc[outlier_indices_consensus, var]\n",
    "    ]\n",
    "    \n",
    "    bp = axes[idx].boxplot(data_to_plot, labels=['Normal', 'Outlier'],\n",
    "                           patch_artist=True, showmeans=True)\n",
    "    \n",
    "    # Colorear cajas\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "    \n",
    "    axes[idx].set_title(f'{var}\\n(Δ: {comparison_stats.loc[var, \"Diferencia_%\"]:.1f}%)', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Comparación de Variables: Outliers vs Normales (Top 8 diferencias)', fontsize=14, y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Outliers Específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinar los outliers más extremos\n",
    "print(\"\\nOUTLIERS MÁS EXTREMOS (Top 10)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Usar distancia PCA como medida de extremidad\n",
    "df_with_pca_dist = df.copy()\n",
    "df_with_pca_dist['PCA_Distance'] = pca_distances\n",
    "df_with_pca_dist['Is_Outlier_Consensus'] = outliers_comparison['Consenso_3+'].values\n",
    "\n",
    "# Top 10 outliers más extremos\n",
    "top_outliers = df_with_pca_dist[df_with_pca_dist['Is_Outlier_Consensus']].nlargest(10, 'PCA_Distance')\n",
    "\n",
    "print(\"\\nTop 10 outliers por distancia PCA:\")\n",
    "print(top_outliers[['fecha', 'PCA_Distance', 'precipitacion_mm', 'temp_media_c', \n",
    "                     'evapotranspiracion_mm', 'humedad_relativa_pct']].to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar un outlier específico en detalle\n",
    "if len(top_outliers) > 0:\n",
    "    # Seleccionar el outlier más extremo\n",
    "    extreme_outlier_idx = top_outliers.index[0]\n",
    "    extreme_outlier_date = top_outliers.iloc[0]['fecha']\n",
    "    \n",
    "    print(f\"\\nANÁLISIS DETALLADO DEL OUTLIER MÁS EXTREMO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Fecha: {extreme_outlier_date}\")\n",
    "    print(f\"Índice: {extreme_outlier_idx}\")\n",
    "    print(f\"Distancia PCA: {pca_distances[extreme_outlier_idx]:.4f}\")\n",
    "    \n",
    "    print(\"\\nValores de las variables:\")\n",
    "    print(df_analysis.loc[extreme_outlier_idx].to_string())\n",
    "    \n",
    "    print(\"\\nZ-Scores de las variables:\")\n",
    "    z_scores_outlier = (df_analysis.loc[extreme_outlier_idx] - df_analysis.mean()) / df_analysis.std()\n",
    "    z_scores_sorted = z_scores_outlier.sort_values(key=abs, ascending=False)\n",
    "    print(z_scores_sorted.head(10).to_string())\n",
    "    \n",
    "    # Visualizar Z-scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(z_scores_sorted)), z_scores_sorted.values,\n",
    "             color=['red' if abs(x) > 3 else 'blue' for x in z_scores_sorted.values],\n",
    "             alpha=0.7, edgecolor='black')\n",
    "    plt.yticks(range(len(z_scores_sorted)), z_scores_sorted.index)\n",
    "    plt.xlabel('Z-Score')\n",
    "    plt.title(f'Z-Scores del Outlier más Extremo ({extreme_outlier_date})')\n",
    "    plt.axvline(x=3, color='red', linestyle='--', alpha=0.5, label='Umbral +3σ')\n",
    "    plt.axvline(x=-3, color='red', linestyle='--', alpha=0.5, label='Umbral -3σ')\n",
    "    plt.axvline(x=0, color='black', linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones y Recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Resumen de Resultados PCA\n",
    "\n",
    "**Análisis de Componentes Principales:**\n",
    "- Se aplicó PCA a 18 variables hidrológicas del dataset (1940-2024)\n",
    "- Las primeras componentes principales capturan la mayoría de la varianza:\n",
    "  - PC1: [Ver resultado en celda anterior]% de varianza\n",
    "  - PC2: [Ver resultado en celda anterior]% de varianza\n",
    "  - PC3: [Ver resultado en celda anterior]% de varianza\n",
    "- Para capturar el 90% de la varianza, se requieren aproximadamente [Ver resultado] componentes\n",
    "\n",
    "**Interpretación de Componentes:**\n",
    "- Las componentes principales revelan patrones de covariación entre variables climáticas\n",
    "- Variables altamente correlacionadas (ej. temperaturas, humedades del suelo) se agrupan en componentes comunes\n",
    "- La reducción dimensional es efectiva, permitiendo representar datos multidimensionales en 2-3 dimensiones\n",
    "\n",
    "### 8.2. Resumen de Detección de Outliers\n",
    "\n",
    "**Métodos Aplicados:**\n",
    "1. **Z-Score:** Detección univariada clásica (umbral 3σ)\n",
    "2. **Isolation Forest:** Algoritmo basado en árboles de decisión\n",
    "3. **Local Outlier Factor (LOF):** Densidad local de vecinos\n",
    "4. **Elliptic Envelope:** Asume distribución Gaussiana multivariada\n",
    "5. **PCA Distance:** Distancia euclidiana en espacio PCA reducido\n",
    "\n",
    "**Resultados:**\n",
    "- Los diferentes métodos detectan entre ~5% de outliers (configurado)\n",
    "- El **consenso (3+ métodos)** identifica outliers más robustos y confiables\n",
    "- Los outliers detectados muestran valores extremos principalmente en:\n",
    "  - [Variables con mayor diferencia - ver análisis previo]\n",
    "\n",
    "### 8.3. Recomendaciones\n",
    "\n",
    "**Para Análisis Futuros:**\n",
    "1. **Investigar outliers por consenso:** Los días identificados por múltiples métodos requieren atención especial\n",
    "2. **Validación temporal:** Verificar si outliers corresponden a eventos meteorológicos extremos conocidos\n",
    "3. **Análisis estacional:** Repetir detección de outliers por estación del año\n",
    "4. **Imputación vs Exclusión:** Decidir si outliers deben ser:\n",
    "   - Excluidos del análisis\n",
    "   - Transformados/imputados\n",
    "   - Analizados por separado como eventos extremos\n",
    "\n",
    "**Para Modelado:**\n",
    "1. **Uso de PCA:** Considerar usar componentes principales en lugar de variables originales para:\n",
    "   - Reducir multicolinealidad\n",
    "   - Simplificar modelos\n",
    "   - Mejorar interpretabilidad\n",
    "2. **Tratamiento de outliers:** Evaluar modelos robustos a outliers o pre-procesamiento específico\n",
    "3. **Segmentación:** Considerar análisis separados para periodos normales vs extremos\n",
    "\n",
    "**Limitaciones:**\n",
    "1. El análisis excluye la variable `almacenamiento_hm3` por valores faltantes\n",
    "2. Los outliers multivariados pueden no ser outliers en variables individuales\n",
    "3. La configuración de umbral de contaminación (5%) es arbitraria y puede ajustarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados para análisis posterior\n",
    "print(\"Guardando resultados...\")\n",
    "\n",
    "# DataFrame con componentes principales\n",
    "df_pca_full = df_pca.copy()\n",
    "df_pca_full.to_csv('../data/processed/pca_components.csv', index=False)\n",
    "print(\"  ✓ Componentes PCA guardadas en: data/processed/pca_components.csv\")\n",
    "\n",
    "# DataFrame con detección de outliers\n",
    "outliers_comparison.to_csv('../data/processed/outliers_detection.csv', index=False)\n",
    "print(\"  ✓ Detección de outliers guardada en: data/processed/outliers_detection.csv\")\n",
    "\n",
    "# Guardar loadings\n",
    "loadings_df.to_csv('../data/processed/pca_loadings.csv', index=True)\n",
    "print(\"  ✓ Loadings PCA guardados en: data/processed/pca_loadings.csv\")\n",
    "\n",
    "print(\"\\n¡Análisis completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN FINAL DEL ANÁLISIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset analizado: datos_hidrologicos_completos.csv\")\n",
    "print(f\"Periodo: {df['fecha'].min()} a {df['fecha'].max()}\")\n",
    "print(f\"Total de observaciones: {len(df):,}\")\n",
    "print(f\"Variables analizadas: {len(numeric_cols)}\")\n",
    "print(f\"\\nPCA:\")\n",
    "print(f\"  - Componentes óptimas (90% varianza): {optimal_components}\")\n",
    "print(f\"  - Reducción dimensional: {len(numeric_cols)} → {optimal_components} ({(1-optimal_components/len(numeric_cols))*100:.1f}% reducción)\")\n",
    "print(f\"\\nOutliers (Consenso 3+ métodos):\")\n",
    "print(f\"  - Total detectados: {outliers_comparison['Consenso_3+'].sum():,} ({outliers_comparison['Consenso_3+'].sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"  - Observaciones normales: {(~outliers_comparison['Consenso_3+']).sum():,} ({(~outliers_comparison['Consenso_3+']).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"\\nArchivos generados:\")\n",
    "print(f\"  - pca_components.csv\")\n",
    "print(f\"  - outliers_detection.csv\")\n",
    "print(f\"  - pca_loadings.csv\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}